# VAE_fake_handwritten_digits

Hi, this demonstrates how to train a VAE model on the MNIST dataset. Unlike a traditional autoencoder, which converts the input image to a latent vector, a VAE model converts the input imgae into a mean a variance. 

By doing that, generating a continuous, structured latent space, which is useful for image generation.

This little unknown project references from a Tensorflow tutorial: https://www.tensorflow.org/tutorials/generative/cvae.

During studying it, I was struggled by this little tricky thing call "R arameterization trick"
